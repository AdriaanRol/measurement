#calculates tail counts per shot for the individual data
#lower: lower bound for tail in nano seconds
#upper: upper bound for tail in nano seconds
import os, sys, time
from matplotlib import pyplot as plt
#from numpy import *
import matplotlib.cm as cm

#plt.close('all')
#current_dir = os.getcwd()

def num2str(num, precision): 
    return "%0.*f" % (precision, num)

def rebin(a, new_binsize):
    #Rebin a 1D numpy array
    noof_bins = len(a)
    if new_binsize > noof_bins:
        print 'Error: binsize exceeds array length'

    b = zeros(ceil(len(a)/float(new_binsize)))
    for k in arange(ceil(len(a)/float(new_binsize))):
        cts_in_bin = 0
        for l in arange(new_binsize):
            if int(k*new_binsize+l) > noof_bins-1:
                break
            else:
                add_terms = a[int(k*new_binsize+l)]
                cts_in_bin += add_terms
        b[k] = cts_in_bin 

    return b

def tail_cts_per_shot(datapath, lower, TPQI_starts, bin_size = 0.256, normalize = False, correct_for_bg = True, save = 1, pulses_in_sequence = 300):
    """Calculates the tail counts per shot, using data generated by the interference setup. Lower specifies the tail begin, TPQI_starts specifies the number of TPQI starts during the data. bin_size in ns, normalize to peak height and option to correct for background"""

    print 'analyzing tail counts per shot...' 
    current_dir = os.getcwd()
    plt.close('all')
    os.chdir(datapath)
    files = os.listdir(datapath)

    for k in arange(len(files)):
        right_file = '.npz' in files[k]
        
        if right_file:
            data = numpy.load(datapath+'\\'+files[k])

    ch1_counts = data['hist_ch1']
    ch0_counts = data['hist_ch0']

    time = bin_size*arange(len(ch1_counts))
    
    if correct_for_bg:
        bg_level_ch1 = ch1_counts[int(0.75*len(ch1_counts)):int(0.90*len(ch1_counts))].mean()
        ch1_counts = ch1_counts - bg_level_ch1*ones(len(ch1_counts))
        bg_level_ch0 = ch0_counts[int(0.75*len(ch0_counts)):int(0.90*len(ch0_counts))].mean()
        ch0_counts = ch0_counts - bg_level_ch0*ones(len(ch0_counts))

        #print 'measured background level for [ch0,ch1] = ['+num2str(bg_level_ch0,1)+','+num2str(bg_level_ch1,1)+']'

    if normalize:
        ch1_counts_normalized = ch1_counts/ch1_counts.max()
        ch0_counts_normalized = ch0_counts/ch0_counts.max()
    
    upper = lower + 40.0

    tail_area_time = time[int(lower/bin_size):int(upper/bin_size)]
    tail_area_ch1 = ch1_counts[int(lower/bin_size):int(upper/bin_size)]
    tail_area_ch0 = ch0_counts[int(lower/bin_size):int(upper/bin_size)]

    tail_counts_per_shot = (tail_area_ch1.sum()+tail_area_ch0.sum())/float(TPQI_starts*pulses_in_sequence)

    figure1 = plt.figure(figsize=(16.0, 12.0))
    plt.subplot(211)
    if not normalize:
        plt.semilogy(time, ch1_counts, '-k')
        plt.plot(array([lower,lower]), array([1E-1,ch1_counts.max()]), 'r', lw = 2.0)
        plt.plot(array([upper,upper]), array([1E-1,ch1_counts.max()]), 'r', lw = 2.0)
    else:
        plt.semilogy(time, ch1_counts_normalized, '-r')
        plt.plot(array([lower,lower]), array([1E-1,ch1_counts_normalized.max()]), 'r', lw = 2.0)
        plt.plot(array([upper,upper]), array([1E-1,ch1_counts_normalized.max()]), 'r', lw = 2.0)
    
    plt.xlabel('Time after sync (ns)')
    plt.ylabel('Counts ch1')
    plt.title('tail counts per shot = '+num2str(tail_counts_per_shot*1e4,1)+'E-4')
    plt.xlim([0,200])

    plt.subplot(212)
    if not normalize:
        plt.semilogy(time, ch0_counts, '-k')
        plt.plot(array([lower,lower]), array([1E-1,ch0_counts.max()]), 'r', lw = 2.0)
        plt.plot(array([upper,upper]), array([1E-1,ch0_counts.max()]), 'r', lw = 2.0)
    else:
        plt.semilogy(time, ch0_counts_normalized, '-k')
        plt.plot(array([lower,lower]), array([1E-1,ch0_counts_normalized.max()]), 'r', lw = 2.0)
        plt.plot(array([upper,upper]), array([1E-1,ch0_counts_normalized.max()]), 'r', lw = 2.0)
    
    plt.xlabel('Time after sync (ns)')
    plt.ylabel('Counts ch0')
    plt.title('tail counts per shot = '+num2str(tail_counts_per_shot*1e4,1)+'E-4')
    plt.xlim([0,200])
    if save:
        figure1.savefig('tail_cts_per_shot.pdf')

    try:
        data.close()
    except:
        pass

    print 'tail counts per shot = '+num2str(tail_counts_per_shot*1e4,1)+'E-4'

    return tail_counts_per_shot



def analyze_thresholds(datapath, threshold_lt1, threshold_lt2, normalize = True, save = 1):
    """Analyzes the thresholds and plots the CR counts during the run in four subplots. Takes threshold values of LT1 and LT2 as input values and has an option to normalize to see the fractional occurence of counts."""
    print 'analyzing thresholds...' 
    current_dir = os.getcwd()
    os.chdir(datapath)
    files = os.listdir(datapath)

    for k in arange(len(files)):
        right_file = '.npz' in files[k]
        
        if right_file:
            data = numpy.load(datapath+'\\'+files[k])
    
    CR_cts_after_seq_lt1 = data['cr_hist_LT1_first']
    CR_cts_after_seq_lt2 = data['cr_hist_LT2_first']

    nr_of_counts = arange(len(CR_cts_after_seq_lt1))

    CR_cts_total_lt1 = data['cr_hist_LT1_total']
    CR_cts_total_lt2 = data['cr_hist_LT2_total']
        
    if normalize:
        CR_cts_after_seq_lt2 = CR_cts_after_seq_lt2/float(sum(CR_cts_after_seq_lt2))
        CR_cts_total_lt2 = CR_cts_total_lt2/float(sum(CR_cts_total_lt2))
        times_passed_after_seq_lt2 = CR_cts_after_seq_lt2[nr_of_counts>=threshold_lt2].sum()*100
        times_passed_overall_lt2 =  CR_cts_total_lt2[nr_of_counts>=threshold_lt2].sum()*100
        
        CR_cts_after_seq_lt1 = CR_cts_after_seq_lt1/float(sum(CR_cts_after_seq_lt1))
        CR_cts_total_lt1 = CR_cts_total_lt1/float(sum(CR_cts_total_lt1))
        times_passed_after_seq_lt1 = CR_cts_after_seq_lt1[nr_of_counts>=threshold_lt1].sum()*100
        times_passed_overall_lt1 =  CR_cts_total_lt1[nr_of_counts>=threshold_lt1].sum()*100
    else:
        times_passed_after_seq_lt2 = CR_cts_after_seq_lt2[nr_of_counts>=threshold_lt2].sum()/float(CR_cts_after_seq_lt2.sum())*100
        times_passed_overall_lt2 =  CR_cts_total_lt2[nr_of_counts>=threshold_lt2].sum()/float(CR_cts_total_lt2.sum())*100
        times_passed_after_seq_lt1 = CR_cts_after_seq_lt1[nr_of_counts>=threshold_lt1].sum()*100/float(CR_cts_after_seq_lt1.sum())
        times_passed_overall_lt1 =  CR_cts_total_lt1[nr_of_counts>=threshold_lt1].sum()*100/float(CR_cts_total_lt1.sum())


        #print 'After sequence: LT2 percentage passed = ',num2str(sum(times_passed_after_seq_lt2),1),'%'
        #print 'and LT1 percentage passed = ',num2str(sum(times_passed_after_seq_lt1),1),'%'

    Log = False

    figure6 = plt.figure(figsize=(16.0, 12.0))
    plt.subplot(223)
    plt.bar(nr_of_counts,CR_cts_after_seq_lt2,log=Log, color = 'm')
    plt.xlabel('Number of counts')
    plt.ylabel('Fraction of occurrences')
    if normalize:
        plt.title('LT2: CR counts after sequence, passed threshold: '+num2str(times_passed_after_seq_lt2,1)+'%')
    else:
        plt.title('CR counts after sequence')
    plt.xlim(0,25)
    
    plt.subplot(224)
    plt.bar(nr_of_counts,CR_cts_total_lt2,log=Log, color = 'm')
    plt.xlabel('Number of counts')
    plt.ylabel('Fraction of occurrences')
    if normalize:
        plt.title('LT2: all CR checks, passed threshold: '+num2str(times_passed_overall_lt2,1)+'%')
    else:
        plt.title('CR counts for all CR checks')
    plt.xlim(0,25)

    plt.subplot(221)
    plt.bar(nr_of_counts,CR_cts_after_seq_lt1,log=Log, color = 'b')
    plt.xlabel('Number of counts')
    plt.ylabel('Fraction of occurrences')
    if normalize:
        plt.title('LT1: CR counts after sequence, passed threshold: '+num2str(times_passed_after_seq_lt1,1)+'%')
    else:
        plt.title('CR counts after sequence')
    plt.xlim(0,50)
    
    plt.subplot(222)
    plt.bar(nr_of_counts,CR_cts_total_lt1,log=Log, color = 'b')
    plt.xlabel('Number of counts')
    plt.ylabel('Fraction of occurrences')
    if normalize:
        plt.title('LT1: all CR checks, passed threshold: '+num2str(times_passed_overall_lt1,1)+'%')
    else:
        plt.title('CR counts for all CR checks')
    plt.xlim(0,50)
    
    if save:
        if normalize:
            figure6.savefig('CR_information_LT1_and_LT2_normalized.pdf')
        else:
            figure6.savefig('CR_information_LT1_and_LT2.pdf')


    return times_passed_overall_lt1, times_passed_after_seq_lt1, times_passed_overall_lt2, times_passed_after_seq_lt2




def analyze_statistics(statsfile):
    statistics = loadtxt(statsfile)
    tpqi_run = statistics[:,0]
    tpqi_starts = statistics[:,5]
    
    return tpqi_starts

    

#TPQI starts = array with TPQI starts.

def analyze_all(datadir, TPQI_starts, dataruns, save = 1, lower = 38.4):
    """Analyzes all the 15 min. measurements in the folder datadir. TPQI starts is an array containing the TPQI starts for all of the 15 min. folders. """
    dirs = os.listdir(datadir)
    idx = 0
    right_dirs = list()


    for l in dataruns:
        for k in arange(len(dirs)):
            mark_right = '_interference_'+num2str(l,0) in dirs[k]
            
            if mark_right and (len(dirs[k]) > len('_interference_'+num2str(l,0))+6):
                mark_right = False

            if mark_right:
                right_dirs.append(dirs[k])
                idx += 1
                continue

                
    if len(right_dirs) == 0:
        print 'Did not find any files'

    if len(dataruns) == len(right_dirs):
        print 'Found all files...'
    else:
        print 'Beware, not all files are taken into account, file(s) missing.'
    
    tail_over_time = zeros(len(right_dirs))
    tpqi_starts = TPQI_starts[dataruns]
    statistics_info = zeros([len(right_dirs),4])
    
    for k in arange(len(right_dirs)):
        tail_over_time[k] = tail_cts_per_shot(datapath = datadir+'\\'+right_dirs[k], lower = lower, TPQI_starts = tpqi_starts[k], save = save)
        statistics_info[k,:] = analyze_thresholds(datapath = datadir+'\\'+right_dirs[k], threshold_lt1 = 0, threshold_lt2 = 9, normalize = True, save = save)


        os.chdir(datadir)
        percentage_finished = float(k+1)/len(right_dirs)*100
        print 'finished: '+num2str(percentage_finished,0)+'%'


    if save:
        times_passed_overall_lt1 = statistics_info[:,0]
        times_passed_after_seq_lt1 = statistics_info[:,1]
        times_passed_overall_lt2 = statistics_info[:,2]
        times_passed_after_seq_lt2 = statistics_info[:,3]
        filename = 'statistics_run_'+num2str(dataruns.min(),0)+'_to_'+num2str(dataruns.max(),0)+'.npz' 
        savez(filename, tpqi_starts = tpqi_starts, tail_over_time = tail_over_time,
                times_passed_overall_lt1 = times_passed_overall_lt1, 
                times_passed_after_seq_lt1 = times_passed_after_seq_lt1, 
                times_passed_overall_lt2 = times_passed_overall_lt2,
                times_passed_after_seq_lt2 = times_passed_after_seq_lt2)

    

    figure3 = plt.figure(figsize=(12.0, 16.0))
    plt.subplot(211)
    plt.plot(dataruns,tail_over_time*1E4, '-k')
    plt.xlabel('TPQI run number')
    plt.ylabel('Tail counts per shot (x 1E-4)')
    plt.grid()
    plt.ylim([0,1.1*max(tail_over_time*1E4)])

    plt.subplot(212)
    plt.plot(dataruns,TPQI_starts[0:len(right_dirs)], '-k')
    plt.xlabel('TPQI run number')
    plt.ylabel('TPQI starts per run')
    plt.grid()
    plt.ylim([0, 1.1*TPQI_starts[0:len(right_dirs)].max()])
    if save:
        figure3.savefig('tpqi_starts_and_tail_over_time.png')

def combine_runs(datapath, runs, do_g2 = True):
    tpqi_starts_allruns = array([])
    tail_cts_per_shot_allruns = array([])
    times_passed_overall_lt1_allruns = array([])
    times_passed_overall_lt2_allruns = array([])
    times_passed_after_seq_lt1_allruns = array([])
    times_passed_after_seq_lt2_allruns = array([])
    counts_in_peaks = 0
    g2_rebinned = 0

    for k in runs:
        npz_file = []
        npz_g2_file = []
        dirs = os.listdir(datapath)
        os.chdir(datapath+'//run'+num2str(k,0))
        
        dirs = os.listdir(os.getcwd())
        for j in range(len(dirs)):
            npz_indicator = dirs[j].startswith('statistics_run') and ('.npz' in dirs[j])
            npz_g2_indicator = dirs[j].startswith('g2_information_run') and dirs[j].endswith('.npz')
            if npz_indicator:
                npz_file = dirs[j]
            if npz_g2_indicator:
                npz_g2_file = dirs[j]

        data_from_runs = load(npz_file)
        
        tpqi_starts_thisrun = data_from_runs['tpqi_starts']
        tail_cts_per_shot_thisrun = data_from_runs['tail_over_time']
        times_passed_overall_lt1_thisrun = data_from_runs['times_passed_overall_lt1']
        times_passed_after_seq_lt1_thisrun = data_from_runs['times_passed_after_seq_lt1']
        times_passed_overall_lt2_thisrun = data_from_runs['times_passed_overall_lt2']
        times_passed_after_seq_lt2_thisrun = data_from_runs['times_passed_after_seq_lt2']

        if do_g2:
            g2_info = load(npz_g2_file)
            counts_in_peaks += g2_info['counts_in_peaks']
            g2_rebinned += g2_info['g2_rebinned']
            dt_rebinned = g2_info['dt_rebinned']
            rebinsize = g2_info['rebinsize']
            peak_numbers = g2_info['peak_numbers']
            g2_info.close

        os.chdir(current_dir)

        tpqi_starts_allruns = concatenate((tpqi_starts_allruns,tpqi_starts_thisrun), axis = 1)
        tail_cts_per_shot_allruns = concatenate((tail_cts_per_shot_allruns, tail_cts_per_shot_thisrun), axis = 1)
        times_passed_overall_lt1_allruns = concatenate((times_passed_overall_lt1_allruns,times_passed_overall_lt1_thisrun), axis = 1)
        times_passed_overall_lt2_allruns = concatenate((times_passed_overall_lt2_allruns,times_passed_overall_lt2_thisrun), axis = 1)
        times_passed_after_seq_lt1_allruns = concatenate((times_passed_after_seq_lt1_allruns,times_passed_after_seq_lt1_thisrun), axis = 1)
        times_passed_after_seq_lt2_allruns = concatenate((times_passed_after_seq_lt2_allruns,times_passed_after_seq_lt2_thisrun), axis = 1)





    tpqi_runs = arange(len(tpqi_starts_allruns))

    print 'total tpqi starts: ',tpqi_starts_allruns.sum()

    figure5 = plt.figure(figsize=(16.0,16.0))
    plt.subplot(321)
    plt.plot(tpqi_runs, tail_cts_per_shot_allruns*1E4, '-or')
    #plt.xlabel('TPQI run number')
    plt.ylabel('Tail counts per shot (x1E-4)')

    plt.subplot(322)
    plt.plot(tpqi_runs, tpqi_starts_allruns, '-or')
    #plt.xlabel('TPQI run number')
    plt.ylabel('TPQI starts in 15 minutes')

    plt.subplot(323)
    plt.plot(tpqi_runs, times_passed_overall_lt1_allruns, '-ob')
    #plt.xlabel('TPQI run number')
    plt.ylabel('Percentage passed of all CR checks')

    plt.subplot(324)
    plt.plot(tpqi_runs, times_passed_after_seq_lt1_allruns, '-ob')
    #plt.xlabel('TPQI run number')
    plt.ylabel('Percentage passed after the sequence')
    plt.legend(['LT 1'])

    plt.subplot(325)
    plt.plot(tpqi_runs, times_passed_overall_lt2_allruns, '-om')
    plt.xlabel('TPQI run number')
    plt.ylabel('Percentage passed of all CR checks')

    plt.subplot(326)
    plt.plot(tpqi_runs, times_passed_after_seq_lt2_allruns, '-om')
    plt.xlabel('TPQI run number')
    plt.ylabel('Percentage passed after the sequence')
    plt.legend(['LT 2'])


    figure5.savefig(datapath+'//measurement_overview.pdf')

    if do_g2:
        err_in_peaks = sqrt(counts_in_peaks)
        contrast_error = sqrt((1/counts_in_peaks[peak_numbers==1]*\
                err_in_peaks[peak_numbers == 0])**2 + \
                (counts_in_peaks[peak_numbers == 0]/counts_in_peaks[peak_numbers == 1]**2\
                *err_in_peaks[peak_numbers == 1])**2)*100
        
        fit_peaks = False

        if fit_peaks:
            data_to_fit = counts_res

            #time_fit = dt
            #counts_fit = 

            #fit_exp_tail = fit.fit_exponential_decay(time_fit,counts_fit, 100, 10)
            #tau = fit_exp_tail['params_dict']['Decay constant']
            #amp = fit_exp_tail['params_dict']['Amplitude']
            #tau_err = fit_exp_tail['error_dict']['Decay constant']
            #amp_err = fit_exp_tail['error_dict']['Amplitude']

            #fit_result = amp*exp(-(time_fit-time_fit[0])/tau)


        figure12 = plt.figure()
        plt.bar(peak_numbers, counts_in_peaks, yerr = err_in_peaks, 
                color = '#6495ED', align = 'center', edgecolor = '#696969',
                ecolor = 'k')
        plt.title('$g^{(2)}(\Delta t)$, contrast = '+\
                num2str(100*(1-counts_in_peaks[peak_numbers==0]\
                /counts_in_peaks[peak_numbers==1]),0)+'$\pm$'+\
                num2str(contrast_error,0)+'%')
        plt.ylabel('Coincidences')
        plt.xlabel('Laser pulse number')

        figure13 = plt.figure()
        plt.bar(dt_rebinned, g2_rebinned, width = rebinsize, align = 'center', color = '#3D9140', edgecolor = '#000000')
        plt.title('$g^{(2)}(\Delta t)$, binwidth = '+num2str(rebinsize,0)+' ns')
        plt.ylabel('Coincidences')
        plt.xlabel('$\Delta t$ (ns)')


        if save:
            figure12.savefig(datapath+'//cumulative_g2_single_bins.pdf')
            figure13.savefig(datapath+'//cumulative_g2_rebinned.pdf')



def g2_from_npzfiles(datapath, dataruns, binsize = 0.256, laser_end = 64.0,
        tail_duration = 50, save = 1, do_ROI = 1, do_rebin = True, rebinsize = 1.024, 
        measurement_rep_time = 200, do_plot_boundaries = False):
    # this function loads data from separate measurements (.npz files) and plots
    # it as a g2, datapath should be the path consisting the 15 min run folders,
    # and dataruns should be an array of numbers that indicate which runs should
    # be taken into account. Ex: for run 0 to 7 use dataruns = arange(0,8) 
    # pulse_end defines the end of the laser pulse, this value is used for filtering

    os.chdir(datapath)
    
    # explanation of parameters: 
    # measurement_rep_time = delay between two laser pulses in ns
    # peak_numbers = which peak numbers should show up in the g2 (ex. arange(-2,3))
    
    shift_boundary_right  = 2 #ns
    peak_numbers = arange(-3,4)
    total_peak_counts = zeros(len(peak_numbers))

    dirs = os.listdir(datapath)
    idx = 0
    right_dirs = list()
    
    # load the data from separate measurements, create a list with the right
    # directories (right_dirs) and load them. relevant data is kept in a new 
    # dictionary (d)

    for l in dataruns:
        for k in arange(len(dirs)):
            mark_right = '_interference_'+num2str(l,0) in dirs[k]
            
            if mark_right and (len(dirs[k]) > len('_interference_'+num2str(l,0))+6):
                mark_right = False

            if mark_right:
                right_dirs.append(dirs[k])
                idx += 1
                continue
              

    if len(dataruns) == len(right_dirs):
        print 'Found all files...'
    elif len(right_dirs) == 0:
        print 'Did not find any files'
    else:
        print 'Beware, not all files are taken into account, file(s) missing.'

    d={}
    print 'Loading data...'

    hist_summed=zeros((4*measurement_rep_time,4000))   # full histogram

    for k in arange(len(right_dirs)):
        data = np.load(right_dirs[k]+'\\'+right_dirs[k]+'.npz')
        noof_dt_bins = len(data['dt'])
        noof_sync_bins = len(data['sync'])
        int_tail = zeros(noof_sync_bins)
        hist_summed+=data['counts']
        summed_counts_single_msmt = data['counts'].sum()
        summed_counts_total= hist_summed.sum()
        if k == 0:
            d['sync'] = data['sync']
            d['dt'] = data['dt']
        data.close()
        print num2str((k+1)/float(len(right_dirs))*100,0)+'%'
            
    print 'total coincidences: %s' %summed_counts_total
    d['counts_summed'] = hist_summed

    
    # plot the 2D histogram with coincidences
    figure9 = plt.figure(figsize=(16.0,9.0))

    im = plt.imshow(d['counts_summed'],origin='lower', 
            interpolation=None, cmap=cm.binary, vmin = 0, vmax=2)
    dt_xticks = linspace(peak_numbers.min()*measurement_rep_time, peak_numbers.max()*measurement_rep_time, \
            len(peak_numbers))/binsize +len(d['dt'])/2
    sync_yticks = linspace(0,measurement_rep_time,6)/binsize
    xtickies = []
    ytickies = []
    
    for l in arange(len(peak_numbers)):
        xtickies.append(peak_numbers[l]*measurement_rep_time)
    for l in arange(len(sync_yticks)):
        ytickies.append(sync_yticks[l]*binsize)
                
    plt.xticks(dt_xticks, xtickies)
    plt.yticks(sync_yticks, ytickies)
    plt.ylabel('Time of ch0 w.r.t. sync (ns)')
    plt.xlabel('$\Delta t$ (ch1-ch0) (ns)')
    cb = figure9.colorbar(im,shrink=0.25,ticks=[2,1,0])
    cb.set_ticklabels(['>2','1','0'])

    if save:
        figure9.savefig('histogram_2D.pdf',format='pdf',dpi=1200)

    
    
    if do_ROI:
        # sync_range contains the range over the sync over which is integrated. So for every dt, the sync range is different. in the loop, n runs over the dt axis, summing all coincidences in the parallellogram for each value of dt. the counts are stored in g2_counts
        print 'Selecting ROI...'

        g2_counts = zeros(noof_dt_bins)

        for peak in peak_numbers:
            tail_start = int((laser_end+measurement_rep_time*peak)/binsize)
            tail_end = tail_start + int(tail_duration/binsize)
            dt_range = arange(+int(shift_boundary_right/binsize)-int(tail_duration/binsize)+int(noof_dt_bins/2)+int(measurement_rep_time*peak/binsize),int(noof_dt_bins/2)+int(measurement_rep_time*peak/binsize)+int(tail_duration/binsize)+int(shift_boundary_right/binsize))
            #print dt_range
            
            sync_range = list()
            sync_range.append(int((laser_end+tail_duration)/binsize)) #initial start of integration
            
            n = 0            
            for k in dt_range:
                if n < len(dt_range)/2:
                    n += 1
                    g2_counts[k] = d['counts_summed'][sync_range,k].sum()
                    sync_range.append(int((laser_end+tail_duration)/binsize-n))
                    if do_plot_boundaries:
                        plt.plot(k,sync_range[len(sync_range)-1],'r.')
                        plt.plot(k,sync_range[0],'r.')
                if (n >= len(dt_range)/2) and (n<len(dt_range)):
                    n += 1
                    g2_counts[k] = d['counts_summed'][sync_range,k].sum()
                    #print d['counts_summed'][sync_range,k]
                    sync_range.remove(sync_range[0])
                    #print sync_range
                    #print n
                    if do_plot_boundaries:
                        plt.plot(k,sync_range[len(sync_range)-1],'r.')
                        plt.plot(k,sync_range[0],'r.')
        
        
        figure10 = plt.figure()
        if do_rebin: #rebin the data using the funcion rebin defined above
            print 'Rebinning...'
            rebin_factor = rebinsize/binsize
            g2_rebinned = rebin(g2_counts,rebin_factor)
            dt_rebinned = arange(-len(g2_rebinned)/2,len(g2_rebinned)/2)*rebinsize
            plt.bar(dt_rebinned, g2_rebinned, width = rebinsize, 
                    align = 'center', color = '#FF4500', edgecolor = '#121212',
                    ecolor = 'k')
            plt.xlim([-1.1*len(g2_rebinned)/2*rebinsize,1.1*len(g2_rebinned)/2*rebinsize])
            plt.ylabel('Coincidences')
            plt.xlabel('$\Delta t$ (ch1-ch0) (ns)')
            plt.title('$g^{(2)}(\Delta t)$, binwidth: '+num2str(rebinsize,1)+' ns')
            #full rebin
            iter = 0
            counts_in_peak = zeros(len(peak_numbers))
            err_in_peak = zeros(len(peak_numbers))
            for peak in peak_numbers:
                peak_start_ns = measurement_rep_time*peak-tail_duration
                peak_end_ns = measurement_rep_time*peak+tail_duration
                single_out_peaks = g2_rebinned[int((peak_start_ns-dt_rebinned[0])/rebinsize):int((peak_end_ns-dt_rebinned[0])/rebinsize)]
                counts_in_peak[iter] = single_out_peaks.sum()
                err_in_peak[iter] = sqrt(single_out_peaks.sum())
                iter += 1
            
            figure11 = plt.figure()
            plt.bar(peak_numbers, counts_in_peak, yerr = err_in_peak, 
                    color = '#6495ED', align = 'center', edgecolor = '#696969',
                    ecolor = 'k')
            plt.title('$g^{(2)}(\Delta t)$')
            plt.ylabel('Coincidences')
            plt.xlabel('Laser pulse number')

            if save:
                figure11.savefig('g2_single_bins.png')
                filename = 'g2_information_run_'+num2str(dataruns.min(),0)+'_to_'+num2str(dataruns.max(),0)+'.npz' 
                savez(filename, dt = arange(-noof_dt_bins/2,noof_dt_bins/2)*binsize, 
                        g2 = g2_counts, 
                        dt_rebinned = dt_rebinned, g2_rebinned = g2_rebinned,
                        peak_numbers = peak_numbers, counts_in_peaks = counts_in_peak,
                        err_in_peaks = err_in_peak,
                        rebinsize = rebinsize, laser_end = laser_end, 
                        tail_duration = tail_duration)


        else: #plot the original, non-rebinned g2
            plt.plot(arange(-noof_dt_bins/2,noof_dt_bins/2)*binsize, g2_counts)
            plt.xlim([-1.1*noof_dt_bins/2*binsize,1.1*noof_dt_bins/2*binsize])
            plt.ylabel('Coincidences')
            plt.xlabel('$\Delta t$ (ch1-ch0) (ns)')     
            plt.title('$g^{(2)}(\Delta t)$, binwidth: '+num2str(binsize,1)+' ns')


        if save:
            figure10.savefig('g2.png')


        print 'Done!'


                





dp = r'O:\THESIS\Python\LT2 setup\20120530\run3\224046_interference_4'
#tail_cts_per_shot(datapath = dp, lower = 39.0, TPQI_starts = 603857, 
#        correct_for_bg = False, save = False)

#analyze_thresholds(datapath = dp, threshold_lt1 = 20, 
#        threshold_lt2 = 9, normalize = True, save = False)

tpqi_starts = analyze_statistics(r'O:\THESIS\Python\LT2 setup\20120530\run3\211918_Statistics_cr_checks\211918_Statistics_cr_checks.dat')

analyze_all(datadir = r'O:\THESIS\Python\LT2 setup\20120530\run3', 
        TPQI_starts = tpqi_starts, dataruns = arange(0,7), save = 1)

#combine_runs(datapath = r'O:\THESIS\Python\LT2 setup\20120521', 
#        runs = arange(1,6), do_g2 = True)

g2_from_npzfiles(datapath = r'O:\THESIS\Python\LT2 setup\20120530\run3', 
        dataruns = arange(0,7), binsize = 0.256, measurement_rep_time = 130,
        do_rebin = True, rebinsize = 1.024*2, laser_end = 37.3, tail_duration = 60, 
        save = True, do_plot_boundaries = False)

#os.chdir(current_dir)
